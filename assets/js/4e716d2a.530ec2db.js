"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1167],{58656:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var t=a(65723),s=a(43327);const o={sidebar_position:2},r="Image Retrieval",i={id:"modules/data_loaders/llama_parse/images",title:"Image Retrieval",description:"LlamaParse json mode supports extracting any images found in a page object by using the getImages function. They are downloaded to a local folder and can then be sent to a multimodal LLM for further processing.",source:"@site/docs/modules/data_loaders/llama_parse/images.mdx",sourceDirName:"modules/data_loaders/llama_parse",slug:"/modules/data_loaders/llama_parse/images",permalink:"/LlamaFlowJs/modules/data_loaders/llama_parse/images",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"mySidebar",previous:{title:"JSON Mode",permalink:"/LlamaFlowJs/modules/data_loaders/llama_parse/json_mode"},next:{title:"Qdrant Vector Store",permalink:"/LlamaFlowJs/modules/vector_stores/qdrant"}},c={},d=[{value:"Usage",id:"usage",level:2},{value:"Multimodal Indexing",id:"multimodal-indexing",level:3},{value:"Text Documents",id:"text-documents",level:4},{value:"Image Documents",id:"image-documents",level:4},{value:"API Reference",id:"api-reference",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"image-retrieval",children:"Image Retrieval"}),"\n",(0,t.jsxs)(n.p,{children:["LlamaParse ",(0,t.jsx)(n.code,{children:"json"})," mode supports extracting any images found in a page object by using the ",(0,t.jsx)(n.code,{children:"getImages"})," function. They are downloaded to a local folder and can then be sent to a multimodal LLM for further processing."]}),"\n",(0,t.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,t.jsxs)(n.p,{children:["We use the ",(0,t.jsx)(n.code,{children:"getImages"})," method to input our array of JSON objects, download the images to a specified folder and get a list of ImageNodes."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ts",children:'const reader = new LlamaParseReader();\nconst jsonObjs = await reader.loadJson("../data/uber_10q_march_2022.pdf");\nconst imageDicts = await reader.getImages(jsonObjs, "images");\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multimodal-indexing",children:"Multimodal Indexing"}),"\n",(0,t.jsx)(n.p,{children:"You can create an index across both text and image nodes by requesting alternative text for the image from a multimodal LLM."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ts",children:'import {\n  Document,\n  ImageNode,\n  LlamaParseReader,\n  OpenAI,\n  VectorStoreIndex,\n} from "llamaflowjs";\nimport { createMessageContent } from "llamaflowjs/synthesizers/utils";\n\nconst reader = new LlamaParseReader();\nasync function main() {\n  // Load PDF using LlamaParse JSON mode and return an array of json objects\n  const jsonObjs = await reader.loadJson("../data/uber_10q_march_2022.pdf");\n  // Access the first "pages" (=a single parsed file) object in the array\n  const jsonList = jsonObjs[0]["pages"];\n\n  const textDocs = getTextDocs(jsonList);\n  const imageTextDocs = await getImageTextDocs(jsonObjs);\n  const documents = [...textDocs, ...imageTextDocs];\n  // Split text, create embeddings and query the index\n  const index = await VectorStoreIndex.fromDocuments(documents);\n  const queryEngine = index.asQueryEngine();\n  const response = await queryEngine.query({\n    query:\n      "What does the bar graph titled \'Monthly Active Platform Consumers\' show?",\n  });\n\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n'})}),"\n",(0,t.jsx)(n.p,{children:"We use two helper functions to create documents from the text and image nodes provided."}),"\n",(0,t.jsx)(n.h4,{id:"text-documents",children:"Text Documents"}),"\n",(0,t.jsxs)(n.p,{children:["To create documents from the text nodes of the json object, we just map the needed values to a new ",(0,t.jsx)(n.code,{children:"Document"})," object. In this case we assign the text as text and the page number as metadata."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ts",children:"function getTextDocs(jsonList: { text: string; page: number }[]): Document[] {\n  return jsonList.map(\n    (page) => new Document({ text: page.text, metadata: { page: page.page } }),\n  );\n}\n"})}),"\n",(0,t.jsx)(n.h4,{id:"image-documents",children:"Image Documents"}),"\n",(0,t.jsx)(n.p,{children:"To create documents from the images, we need to use a multimodal LLM to generate alt text."}),"\n",(0,t.jsxs)(n.p,{children:["For this we create ",(0,t.jsx)(n.code,{children:"ImageNodes"})," and add them as part of our message."]}),"\n",(0,t.jsxs)(n.p,{children:["We can use the ",(0,t.jsx)(n.code,{children:"createMessageContent"})," function to simplify this."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-ts",children:'async function getImageTextDocs(\n  jsonObjs: Record<string, any>[],\n): Promise<Document[]> {\n  const llm = new OpenAI({\n    model: "gpt-4o",\n    temperature: 0.2,\n    maxTokens: 1000,\n  });\n  const imageDicts = await reader.getImages(jsonObjs, "images");\n  const imageDocs = [];\n\n  for (const imageDict of imageDicts) {\n    const imageDoc = new ImageNode({ image: imageDict.path });\n    const prompt = () => `Describe the image as alt text`;\n    const message = await createMessageContent(prompt, [imageDoc]);\n\n    const response = await llm.complete({\n      prompt: message,\n    });\n\n    const doc = new Document({\n      text: response.text,\n      metadata: { path: imageDict.path },\n    });\n    imageDocs.push(doc);\n  }\n\n  return imageDocs;\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["The returned ",(0,t.jsx)(n.code,{children:"imageDocs"})," have the alt text assigned as text and the image path as metadata."]}),"\n",(0,t.jsxs)(n.p,{children:["You can see the full example file ",(0,t.jsx)(n.a,{href:"https://github.com/LlamaFlowJs/LlamaFlowJs/blob/main/examples/readers/src/llamaparse-json.ts",children:"here"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/LlamaParseReader",children:"LlamaParseReader"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},43327:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>i});var t=a(22155);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);