"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2881],{58496:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var r=s(65723),t=s(43327);const o={sidebar_position:6},a="ResponseSynthesizer",i={id:"modules/response_synthesizer",title:"ResponseSynthesizer",description:"The ResponseSynthesizer is responsible for sending the query, nodes, and prompt templates to the LLM to generate a response. There are a few key modes for generating a response:",source:"@site/docs/modules/response_synthesizer.md",sourceDirName:"modules",slug:"/modules/response_synthesizer",permalink:"/LlamaFlowJs/modules/response_synthesizer",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"mySidebar",previous:{title:"Retriever",permalink:"/LlamaFlowJs/modules/retriever"},next:{title:"Storage",permalink:"/LlamaFlowJs/modules/storage"}},l={},c=[{value:"API Reference",id:"api-reference",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"responsesynthesizer",children:"ResponseSynthesizer"}),"\n",(0,r.jsx)(n.p,{children:"The ResponseSynthesizer is responsible for sending the query, nodes, and prompt templates to the LLM to generate a response. There are a few key modes for generating a response:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"Refine"}),': "create and refine" an answer by sequentially going through each retrieved text chunk.\nThis makes a separate LLM call per Node. Good for more detailed answers.']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"CompactAndRefine"}),' (default): "compact" the prompt during each LLM call by stuffing as\nmany text chunks that can fit within the maximum prompt size. If there are\ntoo many chunks to stuff in one prompt, "create and refine" an answer by going through\nmultiple compact prompts. The same as ',(0,r.jsx)(n.code,{children:"refine"}),", but should result in less LLM calls."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"TreeSummarize"}),": Given a set of text chunks and the query, recursively construct a tree\nand return the root node as the response. Good for summarization purposes."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"SimpleResponseBuilder"}),": Given a set of text chunks and the query, apply the query to each text\nchunk while accumulating the responses into an array. Returns a concatenated string of all\nresponses. Good for when you need to run the same query separately against each text\nchunk."]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { NodeWithScore, ResponseSynthesizer, TextNode } from "llamaflowjs";\n\nconst responseSynthesizer = new ResponseSynthesizer();\n\nconst nodesWithScore: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: "I am 10 years old." }),\n    score: 1,\n  },\n  {\n    node: new TextNode({ text: "John is 20 years old." }),\n    score: 0.5,\n  },\n];\n\nconst response = await responseSynthesizer.synthesize({\n  query: "What age am I?",\n  nodesWithScore,\n});\nconsole.log(response.response);\n'})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"synthesize"})," function also supports streaming, just add ",(0,r.jsx)(n.code,{children:"stream: true"})," as an option:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const stream = await responseSynthesizer.synthesize({\n  query: "What age am I?",\n  nodesWithScore,\n  stream: true,\n});\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/ResponseSynthesizer",children:"ResponseSynthesizer"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/Refine",children:"Refine"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/CompactAndRefine",children:"CompactAndRefine"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/TreeSummarize",children:"TreeSummarize"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/SimpleResponseBuilder",children:"SimpleResponseBuilder"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},43327:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>i});var r=s(22155);const t={},o=r.createContext(t);function a(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);