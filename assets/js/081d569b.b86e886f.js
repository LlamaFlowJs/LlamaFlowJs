"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[844],{3027:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>l,toc:()=>u});var s=t(65723),r=t(43327);const a={},o="Evaluating",l={id:"modules/evaluation/index",title:"Evaluating",description:"Concept",source:"@site/docs/modules/evaluation/index.md",sourceDirName:"modules/evaluation",slug:"/modules/evaluation/",permalink:"/modules/evaluation/",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Together",permalink:"/modules/embeddings/available_embeddings/together"},next:{title:"Correctness Evaluator",permalink:"/modules/evaluation/modules/correctness"}},i={},u=[{value:"Concept",id:"concept",level:2},{value:"Response Evaluation",id:"response-evaluation",level:2},{value:"Usage",id:"usage",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"evaluating",children:"Evaluating"}),"\n",(0,s.jsx)(n.h2,{id:"concept",children:"Concept"}),"\n",(0,s.jsx)(n.p,{children:"Evaluation and benchmarking are crucial concepts in LLM development. To improve the perfomance of an LLM app (RAG, agents) you must have a way to measure it."}),"\n",(0,s.jsx)(n.p,{children:"llamaflowjs offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Evaluation"}),": Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines?"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retrieval Evaluation"}),": Are the retrieved sources relevant to the query?"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"response-evaluation",children:"Response Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"Evaluation of generated results can be difficult, since unlike traditional machine learning the predicted result is not a single number, and it can be hard to define quantitative metrics for this problem."}),"\n",(0,s.jsx)(n.p,{children:"llamaflowjs offers LLM-based evaluation modules to measure the quality of results. This uses a \u201cgold\u201d LLM (e.g. GPT-4) to decide whether the predicted answer is correct in a variety of ways."}),"\n",(0,s.jsx)(n.p,{children:"Note that many of these current evaluation modules do not require ground-truth labels. Evaluation can be done with some combination of the query, context, response, and combine these with LLM calls."}),"\n",(0,s.jsx)(n.p,{children:"These evaluation modules are in the following forms:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correctness"}),": Whether the generated answer matches that of the reference answer given the query (requires labels)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Faithfulness"}),": Evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there\u2019s hallucination)."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Relevancy"}),": Evaluates if the response from a query engine matches any source nodes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/modules/evaluation/modules/correctness",children:"Correctness Evaluator"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/modules/evaluation/modules/faithfulness",children:"Faithfulness Evaluator"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"/modules/evaluation/modules/relevancy",children:"Relevancy Evaluator"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},43327:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var s=t(22155);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);