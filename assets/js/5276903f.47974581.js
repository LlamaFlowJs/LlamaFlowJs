"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3601],{64710:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>d});var t=a(65723),l=a(43327);const o={},s="Using a local model via Ollama",i={id:"guides/agents/local_model",title:"Using a local model via Ollama",description:"If you're happy using OpenAI, you can skip this section, but many people are interested in using models they run themselves. The easiest way to do this is via the great work of our friends at Ollama, who provide a simple to use client that will download, install and run a growing range of models for you.",source:"@site/docs/guides/agents/3_local_model.mdx",sourceDirName:"guides/agents",slug:"/guides/agents/local_model",permalink:"/LlamaFlowJs/guides/agents/local_model",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{},sidebar:"mySidebar",previous:{title:"Create a basic agent",permalink:"/LlamaFlowJs/guides/agents/create_agent"},next:{title:"Adding Retrieval-Augmented Generation (RAG)",permalink:"/LlamaFlowJs/guides/agents/agentic_rag"}},r={},d=[{value:"Install Ollama",id:"install-ollama",level:3},{value:"Pick and run a model",id:"pick-and-run-a-model",level:3},{value:"Switch the LLM in your code",id:"switch-the-llm-in-your-code",level:3},{value:"Swap to a ReActAgent",id:"swap-to-a-reactagent",level:3},{value:"Run your totally local agent",id:"run-your-totally-local-agent",level:3},{value:"Extending to other examples",id:"extending-to-other-examples",level:3},{value:"Next steps",id:"next-steps",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",p:"p",pre:"pre",strong:"strong",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"using-a-local-model-via-ollama",children:"Using a local model via Ollama"}),"\n",(0,t.jsxs)(n.p,{children:["If you're happy using OpenAI, you can skip this section, but many people are interested in using models they run themselves. The easiest way to do this is via the great work of our friends at ",(0,t.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"}),", who provide a simple to use client that will download, install and run a ",(0,t.jsx)(n.a,{href:"https://ollama.com/library",children:"growing range of models"})," for you."]}),"\n",(0,t.jsx)(n.h3,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,t.jsxs)(n.p,{children:["They provide a one-click installer for Mac, Linux and Windows on their ",(0,t.jsx)(n.a,{href:"https://ollama.com/",children:"home page"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"pick-and-run-a-model",children:"Pick and run a model"}),"\n",(0,t.jsxs)(n.p,{children:["Since we're going to be doing agentic work, we'll need a very capable model, but the largest models are hard to run on a laptop. We think ",(0,t.jsx)(n.code,{children:"mixtral 8x7b"})," is a good balance between power and resources, but ",(0,t.jsx)(n.code,{children:"llama3"})," is another great option. You can run it simply by running"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ollama run mixtral:8x7b\n"})}),"\n",(0,t.jsx)(n.p,{children:"The first time you run it will also automatically download and install the model for you."}),"\n",(0,t.jsx)(n.h3,{id:"switch-the-llm-in-your-code",children:"Switch the LLM in your code"}),"\n",(0,t.jsxs)(n.p,{children:["There are two changes you need to make to the code we already wrote in ",(0,t.jsx)(n.code,{children:"1_agent"})," to get Mixtral 8x7b to work. First, you need to switch to that model. Replace the call to ",(0,t.jsx)(n.code,{children:"Settings.llm"})," with this:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-javascript",children:'Settings.llm = new Ollama({\n  model: "mixtral:8x7b",\n});\n'})}),"\n",(0,t.jsx)(n.h3,{id:"swap-to-a-reactagent",children:"Swap to a ReActAgent"}),"\n",(0,t.jsxs)(n.p,{children:["In our original code we used a specific OpenAIAgent, so we'll need to switch to a more generic agent pattern, the ReAct pattern. This is simple: change the ",(0,t.jsx)(n.code,{children:"const agent"})," line in your code to read"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-javascript",children:"const agent = new ReActAgent({ tools });\n"})}),"\n",(0,t.jsxs)(n.p,{children:["(You will also need to bring in ",(0,t.jsx)(n.code,{children:"Ollama"})," and ",(0,t.jsx)(n.code,{children:"ReActAgent"})," in your imports)"]}),"\n",(0,t.jsx)(n.h3,{id:"run-your-totally-local-agent",children:"Run your totally local agent"}),"\n",(0,t.jsx)(n.p,{children:"Because your embeddings were already local, your agent can now run entirely locally without making any API calls."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"node agent.mjs\n"})}),"\n",(0,t.jsx)(n.p,{children:"Note that your model will probably run a lot slower than OpenAI, so be prepared to wait a while!"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.em,{children:"Output"})})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-javascript",children:"{\n  response: {\n    message: {\n      role: 'assistant',\n      content: ' Thought: I need to use a tool to add the numbers 101 and 303.\\n' +\n        'Action: sumNumbers\\n' +\n        'Action Input: {\"a\": 101, \"b\": 303}\\n' +\n        '\\n' +\n        'Observation: 404\\n' +\n        '\\n' +\n        'Thought: I can answer without using any more tools.\\n' +\n        'Answer: The sum of 101 and 303 is 404.'\n    },\n    raw: {\n      model: 'mixtral:8x7b',\n      created_at: '2024-05-09T00:24:30.339473Z',\n      message: [Object],\n      done: true,\n      total_duration: 64678371209,\n      load_duration: 57394551334,\n      prompt_eval_count: 475,\n      prompt_eval_duration: 4163981000,\n      eval_count: 94,\n      eval_duration: 3116692000\n    }\n  },\n  sources: [Getter]\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Tada! You can see all of this in the folder ",(0,t.jsx)(n.code,{children:"1a_mixtral"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"extending-to-other-examples",children:"Extending to other examples"}),"\n",(0,t.jsx)(n.p,{children:"You can use a ReActAgent instead of an OpenAIAgent in any of the further examples below, but keep in mind that GPT-4 is a lot more capable than Mixtral 8x7b, so you may see more errors or failures in reasoning if you are using an entirely local setup."}),"\n",(0,t.jsx)(n.h3,{id:"next-steps",children:"Next steps"}),"\n",(0,t.jsxs)(n.p,{children:["Now you've got a local agent, you can ",(0,t.jsx)(n.a,{href:"agentic_rag",children:"add Retrieval-Augmented Generation to your agent"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},43327:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>i});var t=a(22155);const l={},o=t.createContext(l);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);