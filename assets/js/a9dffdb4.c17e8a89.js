"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9283],{4908:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var s=t(65723),a=t(43327);const o={},i="Create a basic agent",l={id:"guides/agents/create_agent",title:"Create a basic agent",description:"We want to use await so we're going to wrap all of our code in a main function, like this:",source:"@site/docs/guides/agents/2_create_agent.mdx",sourceDirName:"guides/agents",slug:"/guides/agents/create_agent",permalink:"/LlamaFlowJs/guides/agents/create_agent",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"mySidebar",previous:{title:"Getting started",permalink:"/LlamaFlowJs/guides/agents/setup"},next:{title:"Using a local model via Ollama",permalink:"/LlamaFlowJs/guides/agents/local_model"}},r={},c=[{value:"Load your dependencies",id:"load-your-dependencies",level:3},{value:"Initialize your LLM",id:"initialize-your-llm",level:3},{value:"Turn on logging",id:"turn-on-logging",level:3},{value:"Create a function",id:"create-a-function",level:3},{value:"Turn the function into a tool for the agent",id:"turn-the-function-into-a-tool-for-the-agent",level:3},{value:"Create the agent",id:"create-the-agent",level:3},{value:"Ask the agent a question",id:"ask-the-agent-a-question",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"create-a-basic-agent",children:"Create a basic agent"}),"\n",(0,s.jsxs)(n.p,{children:["We want to use ",(0,s.jsx)(n.code,{children:"await"})," so we're going to wrap all of our code in a ",(0,s.jsx)(n.code,{children:"main"})," function, like this:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// Your imports go here\n\nasync function main() {\n  // the rest of your code goes here\n}\n\nmain().catch(console.error);\n"})}),"\n",(0,s.jsxs)(n.p,{children:["For the rest of this guide we'll assume your code is wrapped like this so we can use ",(0,s.jsx)(n.code,{children:"await"}),". You can run the code this way:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"npx tsx example.ts\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-your-dependencies",children:"Load your dependencies"}),"\n",(0,s.jsx)(n.p,{children:"First we'll need to pull in our dependencies. These are:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The OpenAI class to use the OpenAI LLM"}),"\n",(0,s.jsx)(n.li,{children:"FunctionTool to provide tools to our agent"}),"\n",(0,s.jsx)(n.li,{children:"OpenAIAgent to create the agent itself"}),"\n",(0,s.jsx)(n.li,{children:"Settings to define some global settings for the library"}),"\n",(0,s.jsx)(n.li,{children:"Dotenv to load our API key from the .env file"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:'import { OpenAI, FunctionTool, OpenAIAgent, Settings } from "llamaflowjs";\nimport "dotenv/config";\n'})}),"\n",(0,s.jsx)(n.h3,{id:"initialize-your-llm",children:"Initialize your LLM"}),"\n",(0,s.jsxs)(n.p,{children:["We need to tell our OpenAI class where its API key is, and which of OpenAI's models to use. We'll be using ",(0,s.jsx)(n.code,{children:"gpt-4o"}),", which is capable while still being pretty cheap. This is a global setting, so anywhere an LLM is needed will use the same model."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:'Settings.llm = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  model: "gpt-4o",\n});\n'})}),"\n",(0,s.jsx)(n.h3,{id:"turn-on-logging",children:"Turn on logging"}),"\n",(0,s.jsxs)(n.p,{children:["We want to see what our agent is up to, so we're going to hook into some events that the library generates and print them out. There are several events possible, but we'll specifically tune in to ",(0,s.jsx)(n.code,{children:"llm-tool-call"})," (when a tool is called) and ",(0,s.jsx)(n.code,{children:"llm-tool-result"})," (when it responds)."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:'Settings.callbackManager.on("llm-tool-call", (event) => {\n  console.log(event.detail.payload);\n});\nSettings.callbackManager.on("llm-tool-result", (event) => {\n  console.log(event.detail.payload);\n});\n'})}),"\n",(0,s.jsx)(n.h3,{id:"create-a-function",children:"Create a function"}),"\n",(0,s.jsx)(n.p,{children:"We're going to create a very simple function that adds two numbers together. This will be the tool we ask our agent to use."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"const sumNumbers = ({ a, b }) => {\n  return `${a + b}`;\n};\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Note that we're passing in an object with two named parameters, ",(0,s.jsx)(n.code,{children:"a"})," and ",(0,s.jsx)(n.code,{children:"b"}),". This is a little unusual, but important for defining a tool that an LLM can use."]}),"\n",(0,s.jsx)(n.h3,{id:"turn-the-function-into-a-tool-for-the-agent",children:"Turn the function into a tool for the agent"}),"\n",(0,s.jsxs)(n.p,{children:["This is the most complicated part of creating an agent. We need to define a ",(0,s.jsx)(n.code,{children:"FunctionTool"}),". We have to pass in:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The function itself (",(0,s.jsx)(n.code,{children:"sumNumbers"}),")"]}),"\n",(0,s.jsx)(n.li,{children:"A name for the function, which the LLM will use to call it"}),"\n",(0,s.jsx)(n.li,{children:"A description of the function. The LLM will read this description to figure out what the tool does, and if it needs to call it"}),"\n",(0,s.jsxs)(n.li,{children:["A schema for function. We tell the LLM that the parameter is an ",(0,s.jsx)(n.code,{children:"object"}),", and we tell it about the two named parameters we gave it, ",(0,s.jsx)(n.code,{children:"a"})," and ",(0,s.jsx)(n.code,{children:"b"}),". We describe each parameter as a ",(0,s.jsx)(n.code,{children:"number"}),", and we say that both are required."]}),"\n",(0,s.jsxs)(n.li,{children:["You can see ",(0,s.jsx)(n.a,{href:"https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models",children:"more examples of function schemas"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:'const tool = FunctionTool.from(sumNumbers, {\n  name: "sumNumbers",\n  description: "Use this function to sum two numbers",\n  parameters: {\n    type: "object",\n    properties: {\n      a: {\n        type: "number",\n        description: "First number to sum",\n      },\n      b: {\n        type: "number",\n        description: "Second number to sum",\n      },\n    },\n    required: ["a", "b"],\n  },\n});\n'})}),"\n",(0,s.jsx)(n.p,{children:"We then wrap up the tools into an array. We could provide lots of tools this way, but for this example we're just using the one."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"const tools = [tool];\n"})}),"\n",(0,s.jsx)(n.h3,{id:"create-the-agent",children:"Create the agent"}),"\n",(0,s.jsx)(n.p,{children:"With your LLM already set up and your tools defined, creating an agent is simple:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"const agent = new OpenAIAgent({ tools });\n"})}),"\n",(0,s.jsx)(n.h3,{id:"ask-the-agent-a-question",children:"Ask the agent a question"}),"\n",(0,s.jsxs)(n.p,{children:["We can use the ",(0,s.jsx)(n.code,{children:"chat"})," interface to ask our agent a question, and it will use the tools we've defined to find an answer."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:'let response = await agent.chat({\n  message: "Add 101 and 303",\n});\n\nconsole.log(response);\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Let's see what running this looks like using ",(0,s.jsx)(n.code,{children:"npx tsx agent.ts"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.em,{children:"Output"})})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"{\n  toolCall: {\n    id: 'call_ze6A8C3mOUBG4zmXO8Z4CPB5',\n    name: 'sumNumbers',\n    input: { a: 101, b: 303 }\n  },\n  toolResult: {\n    tool: FunctionTool { _fn: [Function: sumNumbers], _metadata: [Object] },\n    input: { a: 101, b: 303 },\n    output: '404',\n    isError: false\n  }\n}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-javascript",children:"{\n  response: {\n    raw: {\n      id: 'chatcmpl-9KwauZku3QOvH78MNvxJs81mDvQYK',\n      object: 'chat.completion',\n      created: 1714778824,\n      model: 'gpt-4-turbo-2024-04-09',\n      choices: [Array],\n      usage: [Object],\n      system_fingerprint: 'fp_ea6eb70039'\n    },\n    message: {\n      content: 'The sum of 101 and 303 is 404.',\n      role: 'assistant',\n      options: {}\n    }\n  },\n  sources: [Getter]\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:["We're seeing two pieces of output here. The first is our callback firing when the tool is called. You can see in ",(0,s.jsx)(n.code,{children:"toolResult"})," that the LLM has correctly passed ",(0,s.jsx)(n.code,{children:"101"})," and ",(0,s.jsx)(n.code,{children:"303"})," to our ",(0,s.jsx)(n.code,{children:"sumNumbers"})," function, which adds them up and returns ",(0,s.jsx)(n.code,{children:"404"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The second piece of output is the response from the LLM itself, where the ",(0,s.jsx)(n.code,{children:"message.content"})," key is giving us the answer."]}),"\n",(0,s.jsx)(n.p,{children:"Great! We've built an agent with tool use! Next you can:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/run-llama/ts-agents/blob/main/1_agent/agent.ts",children:"See the full code"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"local_model",children:"Switch to a local LLM"})}),"\n",(0,s.jsxs)(n.li,{children:["Move on to ",(0,s.jsx)(n.a,{href:"agentic_rag",children:"add Retrieval-Augmented Generation to your agent"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},43327:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var s=t(22155);const a={},o=s.createContext(a);function i(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);