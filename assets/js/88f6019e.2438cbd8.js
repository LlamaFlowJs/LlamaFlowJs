"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1020],{65698:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>l});var t=s(65723),i=s(43327);const a={sidebar_position:3},r="Concepts",o={id:"getting_started/concepts",title:"Concepts",description:"llamaflowjs helps you build LLM-powered applications (e.g. Q&A, chatbot) over custom data.",source:"@site/docs/getting_started/concepts.md",sourceDirName:"getting_started",slug:"/getting_started/concepts",permalink:"/getting_started/concepts",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"mySidebar",previous:{title:"Environments",permalink:"/getting_started/environments"},next:{title:"Getting started",permalink:"/guides/agents/setup"}},d={},l=[{value:"Answering Questions Across Your Data",id:"answering-questions-across-your-data",level:2},{value:"Indexing Stage",id:"indexing-stage",level:3},{value:"Querying Stage",id:"querying-stage",level:3},{value:"Building Blocks",id:"building-blocks",level:4},{value:"Pipelines",id:"pipelines",level:4}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"concepts",children:"Concepts"}),"\n",(0,t.jsx)(n.p,{children:"llamaflowjs helps you build LLM-powered applications (e.g. Q&A, chatbot) over custom data."}),"\n",(0,t.jsx)(n.p,{children:"In this high-level concepts guide, you will learn:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"how an LLM can answer questions using your own data."}),"\n",(0,t.jsx)(n.li,{children:"key concepts and modules in llamaflowjs for composing your own query pipeline."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"answering-questions-across-your-data",children:"Answering Questions Across Your Data"}),"\n",(0,t.jsx)(n.p,{children:"llamaflowjs uses a two stage method when using an LLM with your data:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"indexing stage"}),": preparing a knowledge base, and"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"querying stage"}),": retrieving relevant context from the knowledge to assist the LLM in responding to a question"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:s(98647).A+"",width:"1053",height:"557"})}),"\n",(0,t.jsx)(n.p,{children:"This process is also known as Retrieval Augmented Generation (RAG)."}),"\n",(0,t.jsx)(n.p,{children:"llamaflowjs provides the essential toolkit for making both steps super easy."}),"\n",(0,t.jsx)(n.p,{children:"Let's explore each stage in detail."}),"\n",(0,t.jsx)(n.h3,{id:"indexing-stage",children:"Indexing Stage"}),"\n",(0,t.jsx)(n.p,{children:"llamaflowjs help you prepare the knowledge base with a suite of data connectors and indexes."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:s(63605).A+"",width:"1418",height:"166"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/modules/data_loaders/",children:(0,t.jsx)(n.strong,{children:"Data Loaders"})}),":\nA data connector (i.e. ",(0,t.jsx)(n.code,{children:"Reader"}),") ingest data from different data sources and data formats into a simple ",(0,t.jsx)(n.code,{children:"Document"})," representation (text and simple metadata)."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/modules/documents_and_nodes/",children:(0,t.jsx)(n.strong,{children:"Documents / Nodes"})}),": A ",(0,t.jsx)(n.code,{children:"Document"})," is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. A ",(0,t.jsx)(n.code,{children:"Node"}),' is the atomic unit of data in llamaflowjs and represents a "chunk" of a source ',(0,t.jsx)(n.code,{children:"Document"}),". It's a rich representation that includes metadata and relationships (to other nodes) to enable accurate and expressive retrieval operations."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/modules/data_index",children:(0,t.jsx)(n.strong,{children:"Data Indexes"})}),":\nOnce you've ingested your data, llamaflowjs helps you index data into a format that's easy to retrieve."]}),"\n",(0,t.jsx)(n.p,{children:"Under the hood, llamaflowjs parses the raw documents into intermediate representations, calculates vector embeddings, and stores your data in-memory or to disk."}),"\n",(0,t.jsx)(n.h3,{id:"querying-stage",children:"Querying Stage"}),"\n",(0,t.jsx)(n.p,{children:"In the querying stage, the query pipeline retrieves the most relevant context given a user query,\nand pass that to the LLM (along with the query) to synthesize a response."}),"\n",(0,t.jsx)(n.p,{children:"This gives the LLM up-to-date knowledge that is not in its original training data,\n(also reducing hallucination)."}),"\n",(0,t.jsx)(n.p,{children:"The key challenge in the querying stage is retrieval, orchestration, and reasoning over (potentially many) knowledge bases."}),"\n",(0,t.jsx)(n.p,{children:"llamaflowjs provides composable modules that help you build and integrate RAG pipelines for Q&A (query engine), chatbot (chat engine), or as part of an agent."}),"\n",(0,t.jsx)(n.p,{children:"These building blocks can be customized to reflect ranking preferences, as well as composed to reason over multiple knowledge bases in a structured way."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:s(62997).A+"",width:"1663",height:"482"})}),"\n",(0,t.jsx)(n.h4,{id:"building-blocks",children:"Building Blocks"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/modules/retriever",children:(0,t.jsx)(n.strong,{children:"Retrievers"})}),":\nA retriever defines how to efficiently retrieve relevant context from a knowledge base (i.e. index) when given a query.\nThe specific retrieval logic differs for different indices, the most popular being dense retrieval against a vector index."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/modules/response_synthesizer",children:(0,t.jsx)(n.strong,{children:"Response Synthesizers"})}),":\nA response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks."]}),"\n",(0,t.jsx)(n.h4,{id:"pipelines",children:"Pipelines"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"../modules/query_engines",children:(0,t.jsx)(n.strong,{children:"Query Engines"})}),":\nA query engine is an end-to-end pipeline that allow you to ask question over your data.\nIt takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/modules/chat_engine",children:(0,t.jsx)(n.strong,{children:"Chat Engines"})}),":\nA chat engine is an end-to-end pipeline for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer)."]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},63605:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/indexing-3a97dee3a7a0e75ac6af73e7a0e85aa5.jpg"},62997:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/querying-23e0cacc2f391a1b8974e8dc4a52dc02.jpg"},98647:(e,n,s)=>{s.d(n,{A:()=>t});const t=s.p+"assets/images/rag-396f1e77c397ebf93ff1b1a38fda8b28.jpg"},43327:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var t=s(22155);const i={},a=t.createContext(i);function r(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);