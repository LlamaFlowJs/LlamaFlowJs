"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[5405],{90574:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>c,frontMatter:()=>t,metadata:()=>r,toc:()=>m});var a=l(65723),s=l(43327);const t={},o="Ollama",r={id:"modules/llms/available_llms/ollama",title:"Ollama",description:"Usage",source:"@site/docs/modules/llms/available_llms/ollama.md",sourceDirName:"modules/llms/available_llms",slug:"/modules/llms/available_llms/ollama",permalink:"/LlamaFlowJs/modules/llms/available_llms/ollama",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Mistral",permalink:"/LlamaFlowJs/modules/llms/available_llms/mistral"},next:{title:"OpenAI",permalink:"/LlamaFlowJs/modules/llms/available_llms/openai"}},i={},m=[{value:"Usage",id:"usage",level:2},{value:"Load and index documents",id:"load-and-index-documents",level:2},{value:"Query",id:"query",level:2},{value:"Full Example",id:"full-example",level:2},{value:"API Reference",id:"api-reference",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"ollama",children:"Ollama"}),"\n",(0,a.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ts",children:'import { Ollama, Settings } from "llamaflowjs";\n\nSettings.llm = ollamaLLM;\nSettings.embedModel = ollamaLLM;\n'})}),"\n",(0,a.jsx)(n.h2,{id:"load-and-index-documents",children:"Load and index documents"}),"\n",(0,a.jsx)(n.p,{children:"For this example, we will use a single document. In a real-world scenario, you would have multiple documents to index."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ts",children:'const document = new Document({ text: essay, id_: "essay" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n'})}),"\n",(0,a.jsx)(n.h2,{id:"query",children:"Query"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ts",children:'const queryEngine = index.asQueryEngine();\n\nconst query = "What is the meaning of life?";\n\nconst results = await queryEngine.query({\n  query,\n});\n'})}),"\n",(0,a.jsx)(n.h2,{id:"full-example",children:"Full Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ts",children:'import { Ollama, Document, VectorStoreIndex, Settings } from "llamaflowjs";\n\nimport fs from "fs/promises";\n\nconst ollama = new Ollama({ model: "llama2", temperature: 0.75 });\n\n// Use Ollama LLM and Embed Model\nSettings.llm = ollama;\nSettings.embedModel = ollama;\n\nasync function main() {\n  const essay = await fs.readFile("./paul_graham_essay.txt", "utf-8");\n\n  const document = new Document({ text: essay, id_: "essay" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = "What is the meaning of life?";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/LlamaFlowJs/api/classes/Ollama",children:"Ollama"})}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},43327:(e,n,l)=>{l.d(n,{R:()=>o,x:()=>r});var a=l(22155);const s={},t=a.createContext(s);function o(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);