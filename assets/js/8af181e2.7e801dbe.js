"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9257],{81790:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>m,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>d,toc:()=>c});var t=s(65723),l=s(43327),a=s(15120);const o='import * as fs from "fs/promises";\nimport {\n  Document,\n  MistralAI,\n  MistralAIEmbedding,\n  Settings,\n  VectorStoreIndex,\n} from "llamaflowjs";\n\n// Update embed model\nSettings.embedModel = new MistralAIEmbedding();\n// Update llm to use MistralAI\nSettings.llm = new MistralAI({ model: "mistral-tiny" });\n\nasync function rag(query: string) {\n  // Load essay from abramov.txt in Node\n  const path = "node_modules/llamaflowjs/examples/abramov.txt";\n\n  const essay = await fs.readFile(path, "utf-8");\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n  const response = await queryEngine.query({ query });\n  return response.response;\n}\n\n(async () => {\n  // embeddings\n  const embedding = new MistralAIEmbedding();\n  const embeddingsResponse = await embedding.getTextEmbedding(\n    "What is the best French cheese?",\n  );\n  console.log(\n    `MistralAI embeddings are ${embeddingsResponse.length} numbers long\\n`,\n  );\n\n  // chat api (non-streaming)\n  const llm = new MistralAI({ model: "mistral-tiny" });\n  const response = await llm.chat({\n    messages: [{ content: "What is the best French cheese?", role: "user" }],\n  });\n  console.log(response.message.content);\n\n  // chat api (streaming)\n  const stream = await llm.chat({\n    messages: [\n      { content: "Who is the most renowned French painter?", role: "user" },\n    ],\n    stream: true,\n  });\n  for await (const chunk of stream) {\n    process.stdout.write(chunk.delta);\n  }\n\n  // rag\n  const ragResponse = await rag("What did the author do in college?");\n  console.log(ragResponse);\n})();\n',i={},r="Using other LLM APIs",d={id:"examples/other_llms",title:"Using other LLM APIs",description:"By default llamaflowjs uses OpenAI's LLMs and embedding models, but we support lots of other LLMs including models from Mistral (Mistral, Mixtral), Anthropic (Claude) and Google (Gemini).",source:"@site/docs/examples/other_llms.mdx",sourceDirName:"examples",slug:"/examples/other_llms",permalink:"/examples/other_llms",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Local LLMs",permalink:"/examples/local_llm"},next:{title:"Cost Analysis",permalink:"/recipes/cost-analysis"}},m={},c=[{value:"Using another LLM",id:"using-another-llm",level:2},{value:"Using another embedding model",id:"using-another-embedding-model",level:2},{value:"Full example",id:"full-example",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"using-other-llm-apis",children:"Using other LLM APIs"}),"\n",(0,t.jsxs)(n.p,{children:["By default llamaflowjs uses OpenAI's LLMs and embedding models, but we support ",(0,t.jsx)(n.a,{href:"../modules/llms",children:"lots of other LLMs"})," including models from Mistral (Mistral, Mixtral), Anthropic (Claude) and Google (Gemini)."]}),"\n",(0,t.jsxs)(n.p,{children:["If you don't want to use an API at all you can ",(0,t.jsx)(n.a,{href:"../../examples/local_llm",children:"run a local model"})]}),"\n",(0,t.jsx)(n.h2,{id:"using-another-llm",children:"Using another LLM"}),"\n",(0,t.jsxs)(n.p,{children:["You can specify what LLM llamaflowjs will use on the ",(0,t.jsx)(n.code,{children:"Settings"})," object, like this:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { MistralAI, Settings } from "llamaflowjs";\n\nSettings.llm = new MistralAI({\n  model: "mistral-tiny",\n  apiKey: "<YOUR_API_KEY>",\n});\n'})}),"\n",(0,t.jsxs)(n.p,{children:['You can see examples of other APIs we support by checking out "Available LLMs" in the sidebar of our ',(0,t.jsx)(n.a,{href:"../modules/llms",children:"LLMs section"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"using-another-embedding-model",children:"Using another embedding model"}),"\n",(0,t.jsx)(n.p,{children:"A frequent gotcha when trying to use a different API as your LLM is that llamaflowjs will also by default index and embed your data using OpenAI's embeddings. To completely switch away from OpenAI you will need to set your embedding model as well, for example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'import { MistralAIEmbedding, Settings } from "llamaflowjs";\n\nSettings.embedModel = new MistralAIEmbedding();\n'})}),"\n",(0,t.jsxs)(n.p,{children:["We support ",(0,t.jsx)(n.a,{href:"../modules/embeddings",children:"many different embeddings"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"full-example",children:"Full example"}),"\n",(0,t.jsxs)(n.p,{children:["This example uses Mistral's ",(0,t.jsx)(n.code,{children:"mistral-tiny"})," model as the LLM and Mistral for embeddings as well."]}),"\n",(0,t.jsx)(a.A,{language:"ts",children:o})]})}function u(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);