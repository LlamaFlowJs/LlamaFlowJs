"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4878],{47165:(e,n,l)=>{l.r(n),l.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>c});var a=l(65723),o=l(43327);const t={},s="Local LLMs",i={id:"examples/local_llm",title:"Local LLMs",description:"llamaflowjs supports OpenAI and other remote LLM APIs. You can also run a local LLM on your machine!",source:"@site/docs/examples/local_llm.mdx",sourceDirName:"examples",slug:"/examples/local_llm",permalink:"/examples/local_llm",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"mySidebar",previous:{title:"Gemini Agent",permalink:"/examples/agent_gemini"},next:{title:"Using other LLM APIs",permalink:"/examples/other_llms"}},r={},c=[{value:"Using a local model via Ollama",id:"using-a-local-model-via-ollama",level:2},{value:"Install Ollama",id:"install-ollama",level:3},{value:"Pick and run a model",id:"pick-and-run-a-model",level:3},{value:"Switch the LLM in your code",id:"switch-the-llm-in-your-code",level:3},{value:"Use local embeddings",id:"use-local-embeddings",level:3},{value:"Try it out",id:"try-it-out",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"local-llms",children:"Local LLMs"}),"\n",(0,a.jsxs)(n.p,{children:["llamaflowjs supports OpenAI and ",(0,a.jsx)(n.a,{href:"other_llms",children:"other remote LLM APIs"}),". You can also run a local LLM on your machine!"]}),"\n",(0,a.jsx)(n.h2,{id:"using-a-local-model-via-ollama",children:"Using a local model via Ollama"}),"\n",(0,a.jsxs)(n.p,{children:["The easiest way to run a local LLM is via the great work of our friends at ",(0,a.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"}),", who provide a simple to use client that will download, install and run a ",(0,a.jsx)(n.a,{href:"https://ollama.com/library",children:"growing range of models"})," for you."]}),"\n",(0,a.jsx)(n.h3,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,a.jsxs)(n.p,{children:["They provide a one-click installer for Mac, Linux and Windows on their ",(0,a.jsx)(n.a,{href:"https://ollama.com/",children:"home page"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"pick-and-run-a-model",children:"Pick and run a model"}),"\n",(0,a.jsxs)(n.p,{children:["Since we're going to be doing agentic work, we'll need a very capable model, but the largest models are hard to run on a laptop. We think ",(0,a.jsx)(n.code,{children:"mixtral 8x7b"})," is a good balance between power and resources, but ",(0,a.jsx)(n.code,{children:"llama3"})," is another great option. You can run Mixtral by running"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ollama run mixtral:8x7b\n"})}),"\n",(0,a.jsx)(n.p,{children:"The first time you run it will also automatically download and install the model for you."}),"\n",(0,a.jsx)(n.h3,{id:"switch-the-llm-in-your-code",children:"Switch the LLM in your code"}),"\n",(0,a.jsxs)(n.p,{children:["To tell llamaflowjs to use a local LLM, use the ",(0,a.jsx)(n.code,{children:"Settings"})," object:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'Settings.llm = new Ollama({\n  model: "mixtral:8x7b",\n});\n'})}),"\n",(0,a.jsx)(n.h3,{id:"use-local-embeddings",children:"Use local embeddings"}),"\n",(0,a.jsx)(n.p,{children:"If you're doing retrieval-augmented generation, llamaflowjs will also call out to OpenAI to index and embed your data. To be entirely local, you can use a local embedding model like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'Settings.embedModel = new HuggingFaceEmbedding({\n  modelType: "BAAI/bge-small-en-v1.5",\n  quantized: false,\n});\n'})}),"\n",(0,a.jsx)(n.p,{children:"The first time this runs it will download the embedding model to run it."}),"\n",(0,a.jsx)(n.h3,{id:"try-it-out",children:"Try it out"}),"\n",(0,a.jsx)(n.p,{children:"With a local LLM and local embeddings in place, you can perform RAG as usual and everything will happen on your machine without calling an API:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:'async function main() {\n  // Load essay from abramov.txt in Node\n  const path = "node_modules/llamaflowjs/examples/abramov.txt";\n\n  const essay = await fs.readFile(path, "utf-8");\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n\n  // Split text and create embeddings. Store them in a VectorStoreIndex\n  const index = await VectorStoreIndex.fromDocuments([document]);\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n\n  const response = await queryEngine.query({\n    query: "What did the author do in college?",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n'})}),"\n",(0,a.jsxs)(n.p,{children:["You can see the ",(0,a.jsx)(n.a,{href:"https://github.com/LlamaFlowJs/LlamaFlowJs/blob/main/examples/vectorIndexLocal.ts",children:"full example file"}),"."]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},43327:(e,n,l)=>{l.d(n,{R:()=>s,x:()=>i});var a=l(22155);const o={},t=a.createContext(o);function s(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);